{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":6068,"sourceType":"modelInstanceVersion","modelInstanceId":4689,"modelId":2821}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem\n\nThe challenge is to train a model that can identify whether a tweet is about a disaster or not. In order to get the data ready some basic NLP tasks will be conducted such as lower casing, dropping numbers and punctuations. And then the data will be tokenized, converted into a sequence and some padding will be applied to ensure uniform input.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load train and test datasets into DataFrames\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# Display the first few rows of each DataFrame to verify the data\nprint(train_df.head())\nprint(test_df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-15T17:51:39.345497Z","iopub.execute_input":"2024-09-15T17:51:39.345964Z","iopub.status.idle":"2024-09-15T17:51:39.779494Z","shell.execute_reply.started":"2024-09-15T17:51:39.345918Z","shell.execute_reply":"2024-09-15T17:51:39.778570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA of the Data","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\n\n# Creating a word cloud for disaster and non-disaster tweets\ndisaster_tweets = \" \".join(train_df_filtered[train_df_filtered['target'] == 1]['text'])\nnondisaster_tweets = \" \".join(train_df_filtered[train_df_filtered['target'] == 0]['text'])\n\n# WordCloud for disaster tweets\ndisaster_wordcloud = WordCloud(width=800, height=400, max_words=100, background_color=\"white\").generate(disaster_tweets)\n\n# WordCloud for non-disaster tweets\nnondisaster_wordcloud = WordCloud(width=800, height=400, max_words=100, background_color=\"white\").generate(nondisaster_tweets)\n\n# Plotting the WordClouds\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.imshow(disaster_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"WordCloud for Disaster Tweets\", fontsize=16)\n\nplt.subplot(1, 2, 2)\nplt.imshow(nondisaster_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"WordCloud for Non-Disaster Tweets\", fontsize=16)\n\nplt.tight_layout()\nplt.show()\n\n# Creating a count plot for the tweet distribution by target\nplt.figure(figsize=(8, 6))\nsns.countplot(x=train_df_filtered['target'])\nplt.title('Tweet Count Distribution by Target', fontsize=16)\nplt.xlabel('Target (0 = Non-disaster, 1 = Disaster)', fontsize=12)\nplt.ylabel('Tweet Count', fontsize=12)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T17:54:03.424640Z","iopub.execute_input":"2024-09-15T17:54:03.425555Z","iopub.status.idle":"2024-09-15T17:54:06.559510Z","shell.execute_reply.started":"2024-09-15T17:54:03.425491Z","shell.execute_reply":"2024-09-15T17:54:06.558655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Structure\n\nThe data has an ID, keyword, location and Text columns. Since the keyword and location has some nulls, they are dropped. So the ID, Text and Target columns are used for training. ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Filter the train and test DataFrames to only use the 'id' and 'text' columns\ntrain_df_filtered = train_df[['id', 'text', 'target']]\ntest_df_filtered = test_df[['id', 'text']]\n\n# Display the first few rows of the filtered DataFrames to verify\nprint(train_df_filtered.head())\nprint(test_df_filtered.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T17:51:42.062907Z","iopub.execute_input":"2024-09-15T17:51:42.063253Z","iopub.status.idle":"2024-09-15T17:51:42.079904Z","shell.execute_reply.started":"2024-09-15T17:51:42.063220Z","shell.execute_reply":"2024-09-15T17:51:42.078702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Preprocessing Steps","metadata":{}},{"cell_type":"code","source":"import re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Function to clean the text\ndef clean_text(text):\n    text = text.lower()  # Convert text to lowercase\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and special characters\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    return text\n\n# Clean the text in both the train and test datasets\ntrain_df_filtered['text_cleaned'] = train_df_filtered['text'].apply(clean_text)\ntest_df_filtered['text_cleaned'] = test_df_filtered['text'].apply(clean_text)\n\n# Tokenizing the text\ntokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\ntokenizer.fit_on_texts(train_df_filtered['text_cleaned'])\n\n# Converting text to sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_df_filtered['text_cleaned'])\ntest_sequences = tokenizer.texts_to_sequences(test_df_filtered['text_cleaned'])\n\n# Padding the sequences to ensure uniform input size\nmax_length = 100  # You can experiment with this value\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n\n# Display the first few sequences\nprint(train_padded[:5])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T02:43:38.136497Z","iopub.execute_input":"2024-09-15T02:43:38.136907Z","iopub.status.idle":"2024-09-15T02:43:38.725156Z","shell.execute_reply.started":"2024-09-15T02:43:38.136869Z","shell.execute_reply":"2024-09-15T02:43:38.724163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df_filtered['target'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T02:37:22.857593Z","iopub.execute_input":"2024-09-15T02:37:22.857916Z","iopub.status.idle":"2024-09-15T02:37:22.863548Z","shell.execute_reply.started":"2024-09-15T02:37:22.857883Z","shell.execute_reply":"2024-09-15T02:37:22.862530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN Model\n\nWe first try this approach since RNNs are good for sequential tasks such as text processing. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.initializers import HeNormal\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\n\n# Define the model architecture\nvocab_size = 10000  # Same as the tokenizer's num_words parameter\nembedding_dim = 16\nmax_length = 100  # This should match the max_length used for padding\n\nmodel = Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=max_length, embeddings_initializer=HeNormal()),\n    Bidirectional(LSTM(64, return_sequences=False, kernel_initializer=HeNormal(), kernel_regularizer=l2(0.01))),  # Add L2 regularization\n    Dropout(0.5),\n    BatchNormalization(),\n    Dense(32, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.01)),  # Add L2 regularization\n    Dropout(0.5),\n    Dense(1, activation='sigmoid', kernel_initializer=HeNormal(), kernel_regularizer=l2(0.01))  # Add L2 regularization\n])\n\n# Adjust the learning rate\noptimizer = Adam(learning_rate=0.00001)  # Adjust to a higher value\n\n# Compile the model with binary_crossentropy loss\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T02:37:22.864933Z","iopub.execute_input":"2024-09-15T02:37:22.865247Z","iopub.status.idle":"2024-09-15T02:37:22.924270Z","shell.execute_reply.started":"2024-09-15T02:37:22.865212Z","shell.execute_reply":"2024-09-15T02:37:22.923406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df_filtered['target'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T02:37:22.926128Z","iopub.execute_input":"2024-09-15T02:37:22.926490Z","iopub.status.idle":"2024-09-15T02:37:22.932190Z","shell.execute_reply.started":"2024-09-15T02:37:22.926452Z","shell.execute_reply":"2024-09-15T02:37:22.931177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np  # Add this line to import numpy\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Split the training data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(train_padded, train_df_filtered['target'], test_size=0.2, random_state=42)\n\n# Convert the target columns to numpy arrays\ny_train = np.array(y_train)\ny_val = np.array(y_val)\n\n# Calculate class weights to handle potential class imbalance\nclass_weights_array = class_weight.compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_df_filtered['target']),\n    y=train_df_filtered['target']\n)\n\n# Create the class weight dictionary based on the target labels (0 and 1)\nclass_weights = {0: class_weights_array[0], 1: class_weights_array[1]}\n\n# Early stopping callback\nearly_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\n\n# Train the model on the dataset\nhistory = model.fit(\n    X_train, y_train,\n    epochs=35,\n    batch_size=64,  # You can adjust this as needed\n    validation_data=(X_val, y_val),\n    class_weight=class_weights,  # Apply class weights to handle class imbalance\n    callbacks=[early_stop],  # Early stopping callback\n    verbose=2\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T02:37:22.933544Z","iopub.execute_input":"2024-09-15T02:37:22.933894Z","iopub.status.idle":"2024-09-15T02:38:08.220351Z","shell.execute_reply.started":"2024-09-15T02:37:22.933857Z","shell.execute_reply":"2024-09-15T02:38:08.219576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\nThe RNN model performed quite well and achieved good validation accuracy. 35 epochs were identified to be ideal as after that the validation accuracy was decreasing while training accuracy increased.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot accuracy over epochs\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()\n\n# Plot loss over epochs\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T02:38:08.222933Z","iopub.execute_input":"2024-09-15T02:38:08.223634Z","iopub.status.idle":"2024-09-15T02:38:08.782180Z","shell.execute_reply.started":"2024-09-15T02:38:08.223598Z","shell.execute_reply":"2024-09-15T02:38:08.781222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize and pad the test data\ntest_sequences = tokenizer.texts_to_sequences(test_df_filtered['text_cleaned'])  # Ensure you have cleaned the text the same way\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n\n# Predict using the trained model\npredictions = model.predict(test_padded)\n\n# Convert predictions to binary output (0 or 1)\npredicted_labels = (predictions > 0.5).astype(int)  # Since itâ€™s a binary classification, threshold is 0.5\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T02:40:29.370591Z","iopub.execute_input":"2024-09-15T02:40:29.371016Z","iopub.status.idle":"2024-09-15T02:40:30.439826Z","shell.execute_reply.started":"2024-09-15T02:40:29.370976Z","shell.execute_reply":"2024-09-15T02:40:30.438530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with the ID and predicted target\nsubmission_df = pd.DataFrame({\n    'id': test_df_filtered['id'],  # Use the ID from the test dataset\n    'target': predicted_labels.flatten()  # Flatten if needed, so it's in the correct shape\n})\n\n# Save the submission file as a CSV\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Submission file created successfully!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T02:40:37.507441Z","iopub.execute_input":"2024-09-15T02:40:37.508003Z","iopub.status.idle":"2024-09-15T02:40:37.525776Z","shell.execute_reply.started":"2024-09-15T02:40:37.507963Z","shell.execute_reply":"2024-09-15T02:40:37.524856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring with a Distil Bert","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load train and test datasets into DataFrames\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# Display the first few rows of each DataFrame to verify the data\nprint(train_df.head())\nprint(test_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:48:10.837950Z","iopub.execute_input":"2024-09-15T03:48:10.838343Z","iopub.status.idle":"2024-09-15T03:48:10.888647Z","shell.execute_reply.started":"2024-09-15T03:48:10.838305Z","shell.execute_reply":"2024-09-15T03:48:10.887617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the train and test DataFrames to only use the 'id' and 'text' columns\ntrain_df_filtered = train_df[['id', 'text', 'target']]\ntest_df_filtered = test_df[['id', 'text']]\n\n# Display the first few rows of the filtered DataFrames to verify\nprint(train_df_filtered.head())\nprint(test_df_filtered.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:48:11.057683Z","iopub.execute_input":"2024-09-15T03:48:11.058017Z","iopub.status.idle":"2024-09-15T03:48:11.070225Z","shell.execute_reply.started":"2024-09-15T03:48:11.057982Z","shell.execute_reply":"2024-09-15T03:48:11.069264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras_nlp\nimport keras\nfrom tensorflow.keras.optimizers import Adam\n\n# Define some hyperparameters\npreset = \"distil_bert_base_en_uncased\"\nsequence_length = 160\nBATCH_SIZE = 16\nEPOCHS = 3\n\n# Load a DistilBERT preprocessor with a sequence length of 160\npreprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(\n    preset,\n    sequence_length=sequence_length,\n    name=\"preprocessor_4_tweets\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:27:48.762480Z","iopub.execute_input":"2024-09-15T03:27:48.762862Z","iopub.status.idle":"2024-09-15T03:27:52.332443Z","shell.execute_reply.started":"2024-09-15T03:27:48.762817Z","shell.execute_reply":"2024-09-15T03:27:52.331615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess text\nimport re\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\@w+|\\#', '', text)\n    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Apply preprocessing to training and test data\ntrain_df_filtered['text_cleaned'] = train_df_filtered['text'].apply(preprocess_text)\ntest_df_filtered['text_cleaned'] = test_df_filtered['text'].apply(preprocess_text)\n\n# Function to tokenize the text using DistilBERT tokenizer\ndef encode_data(texts, tokenizer, max_length=128):\n    encoded = tokenizer.batch_encode_plus(\n        texts.tolist(),  # Convert the text column to a list\n        add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n        max_length=max_length,  # Max sequence length\n        padding='max_length',  # Pad sequences to max_length\n        truncation=True,  # Truncate longer sequences\n        return_attention_mask=True,  # Return attention mask\n        return_tensors='tf'  # Return TensorFlow tensors\n    )\n    return encoded['input_ids'], encoded['attention_mask']\n\n# Tokenize the training and test data\nX_train_input_ids, X_train_attention_mask = encode_data(train_df_filtered['text_cleaned'], tokenizer)\nX_test_input_ids, X_test_attention_mask = encode_data(test_df_filtered['text_cleaned'], tokenizer)\n\n# Display the shape of the tokenized data to verify\nprint(X_train_input_ids.shape)\nprint(X_test_input_ids.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:48:30.987287Z","iopub.execute_input":"2024-09-15T03:48:30.988096Z","iopub.status.idle":"2024-09-15T03:48:38.176823Z","shell.execute_reply.started":"2024-09-15T03:48:30.988053Z","shell.execute_reply":"2024-09-15T03:48:38.175789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_filtered = train_df_filtered.iloc[:len(y_train)]\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:27:59.516474Z","iopub.execute_input":"2024-09-15T03:27:59.516815Z","iopub.status.idle":"2024-09-15T03:27:59.521904Z","shell.execute_reply.started":"2024-09-15T03:27:59.516779Z","shell.execute_reply":"2024-09-15T03:27:59.520925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the DistilBERT classifier with 2 output classes for binary classification\nclassifier = keras_nlp.models.DistilBertClassifier.from_preset(\n    preset,\n    preprocessor=preprocessor,\n    num_classes=2\n)\n\n# Display model summary\nclassifier.summary()\n\n# Compile the model\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Suitable for integer labels\n    optimizer=Adam(learning_rate=1e-5),\n    metrics=[\"accuracy\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:27:59.523471Z","iopub.execute_input":"2024-09-15T03:27:59.524403Z","iopub.status.idle":"2024-09-15T03:28:07.309346Z","shell.execute_reply.started":"2024-09-15T03:27:59.524335Z","shell.execute_reply":"2024-09-15T03:28:07.308422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_df_filtered['text_cleaned']))\nprint(len(y_train))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:28:07.310646Z","iopub.execute_input":"2024-09-15T03:28:07.311059Z","iopub.status.idle":"2024-09-15T03:28:07.316873Z","shell.execute_reply.started":"2024-09-15T03:28:07.311011Z","shell.execute_reply":"2024-09-15T03:28:07.315894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the training data into training and validation sets\nX_train_texts, X_val_texts, y_train, y_val = train_test_split(\n    train_df_filtered['text_cleaned'], y_train, test_size=0.2, random_state=42\n)\n\n# Train the model using raw text (no need for manual tokenization)\nhistory = classifier.fit(\n    x=X_train_texts,  # Use the cleaned text directly for training\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(X_val_texts, y_val)  # Use the validation data split from training set\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:28:07.318036Z","iopub.execute_input":"2024-09-15T03:28:07.318443Z","iopub.status.idle":"2024-09-15T03:32:04.620579Z","shell.execute_reply.started":"2024-09-15T03:28:07.318399Z","shell.execute_reply":"2024-09-15T03:32:04.619660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the training and validation accuracy\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:32:04.623017Z","iopub.execute_input":"2024-09-15T03:32:04.623347Z","iopub.status.idle":"2024-09-15T03:32:05.232081Z","shell.execute_reply.started":"2024-09-15T03:32:04.623311Z","shell.execute_reply":"2024-09-15T03:32:05.231175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Predict the validation data\ny_val_pred = classifier.predict(X_val_texts)\ny_val_pred_labels = y_val_pred.argmax(axis=1)  # Get the predicted labels\n\n# Generate the confusion matrix\nconf_matrix = confusion_matrix(y_val, y_val_pred_labels)\n\n# Plot the confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:32:05.233231Z","iopub.execute_input":"2024-09-15T03:32:05.233523Z","iopub.status.idle":"2024-09-15T03:32:15.556906Z","shell.execute_reply.started":"2024-09-15T03:32:05.233492Z","shell.execute_reply":"2024-09-15T03:32:15.555967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the test data (X_test_texts is the raw text from the test set)\ny_test_pred = classifier.predict(test_df_filtered['text_cleaned'])\ny_test_pred_labels = y_test_pred.argmax(axis=1)  # Get the predicted labels\n\n# Create a submission file\nsubmission_df = test_df_filtered[['id']].copy()\nsubmission_df['target'] = y_test_pred_labels\n\n# Save the submission file\nsubmission_df.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:32:15.558232Z","iopub.execute_input":"2024-09-15T03:32:15.558564Z","iopub.status.idle":"2024-09-15T03:32:31.166459Z","shell.execute_reply.started":"2024-09-15T03:32:15.558529Z","shell.execute_reply":"2024-09-15T03:32:31.165582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring with pre-trained BERT model for binary classification","metadata":{}},{"cell_type":"code","source":"# Filter the train and test DataFrames to only use the 'id' and 'text' columns\ntrain_df_filtered = train_df[['id', 'text', 'target']]\ntest_df_filtered = test_df[['id', 'text']]\n\n# Display the first few rows of the filtered DataFrames to verify\nprint(train_df_filtered.head())\nprint(test_df_filtered.head())\n# Filter the train and test DataFrames to only use the 'id' and 'text' columns\ntrain_df_filtered = train_df[['id', 'text', 'target']]\ntest_df_filtered = test_df[['id', 'text']]\n\n# Display the first few rows of the filtered DataFrames to verify\nprint(train_df_filtered.head())\nprint(test_df_filtered.head())\n# Function to preprocess text\nimport re\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\@w+|\\#', '', text)\n    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Apply preprocessing to training and test data\ntrain_df_filtered['text_cleaned'] = train_df_filtered['text'].apply(preprocess_text)\ntest_df_filtered['text_cleaned'] = test_df_filtered['text'].apply(preprocess_text)\n\n# Function to tokenize the text using DistilBERT tokenizer\ndef encode_data(texts, tokenizer, max_length=128):\n    encoded = tokenizer.batch_encode_plus(\n        texts.tolist(),  # Convert the text column to a list\n        add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n        max_length=max_length,  # Max sequence length\n        padding='max_length',  # Pad sequences to max_length\n        truncation=True,  # Truncate longer sequences\n        return_attention_mask=True,  # Return attention mask\n        return_tensors='tf'  # Return TensorFlow tensors\n    )\n    return encoded['input_ids'], encoded['attention_mask']\n\n# Tokenize the training and test data\nX_train_input_ids, X_train_attention_mask = encode_data(train_df_filtered['text_cleaned'], tokenizer)\nX_test_input_ids, X_test_attention_mask = encode_data(test_df_filtered['text_cleaned'], tokenizer)\n\n# Display the shape of the tokenized data to verify\nprint(X_train_input_ids.shape)\nprint(X_test_input_ids.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T03:59:35.047340Z","iopub.execute_input":"2024-09-15T03:59:35.047822Z","iopub.status.idle":"2024-09-15T03:59:42.262301Z","shell.execute_reply.started":"2024-09-15T03:59:35.047781Z","shell.execute_reply":"2024-09-15T03:59:42.261107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertForSequenceClassification, BertTokenizer\nimport tensorflow as tf\n\n# Load\nbert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:00:27.679430Z","iopub.execute_input":"2024-09-15T04:00:27.679828Z","iopub.status.idle":"2024-09-15T04:00:28.917419Z","shell.execute_reply.started":"2024-09-15T04:00:27.679789Z","shell.execute_reply":"2024-09-15T04:00:28.916480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\n# Convert TensorFlow tensors to numpy arrays\nX_train_input_ids_np = X_train_input_ids.numpy()\nX_train_attention_mask_np = X_train_attention_mask.numpy()\n\n# Now split the numpy arrays\nX_train_input_ids_np, X_val_input_ids_np, X_train_attention_mask_np, X_val_attention_mask_np, y_train, y_val = train_test_split(\n    X_train_input_ids_np, X_train_attention_mask_np, train_df_filtered['target'], test_size=0.2, random_state=42\n)\n\n# Convert target variables to TensorFlow tensors\ny_train = tf.convert_to_tensor(y_train.values)\ny_val = tf.convert_to_tensor(y_val.values)\n\n# If needed, convert the numpy arrays back to TensorFlow tensors\nX_train_input_ids = tf.convert_to_tensor(X_train_input_ids_np)\nX_val_input_ids = tf.convert_to_tensor(X_val_input_ids_np)\nX_train_attention_mask = tf.convert_to_tensor(X_train_attention_mask_np)\nX_val_attention_mask = tf.convert_to_tensor(X_val_attention_mask_np)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:00:56.763832Z","iopub.execute_input":"2024-09-15T04:00:56.764255Z","iopub.status.idle":"2024-09-15T04:00:56.779787Z","shell.execute_reply.started":"2024-09-15T04:00:56.764214Z","shell.execute_reply":"2024-09-15T04:00:56.778713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertForSequenceClassification\nimport numpy as np\n\n# Load the pre-trained BERT model for sequence classification\nbert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Define optimizer and loss function\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Prepare dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': X_train_input_ids, 'attention_mask': X_train_attention_mask},\n    y_train\n)).shuffle(100).batch(16)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': X_val_input_ids, 'attention_mask': X_val_attention_mask},\n    y_val\n)).batch(16)\n\n# Training loop\nepochs = 3\n\nfor epoch in range(epochs):\n    print(f'Epoch {epoch + 1}/{epochs}')\n    for step, (inputs, labels) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            logits = bert_model(inputs, training=True).logits\n            loss = loss_fn(labels, logits)\n        \n        # Backpropagation\n        gradients = tape.gradient(loss, bert_model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, bert_model.trainable_variables))\n        \n        if step % 50 == 0:\n            print(f'Step {step}, Loss: {loss.numpy()}')\n\n    # Validation\n    val_loss = []\n    val_acc = []\n    for inputs, labels in val_dataset:\n        val_logits = bert_model(inputs, training=False).logits\n        val_loss.append(loss_fn(labels, val_logits).numpy())\n        \n        # Calculate accuracy\n        predictions = np.argmax(val_logits, axis=-1)\n        accuracy = np.mean(predictions == labels.numpy())\n        val_acc.append(accuracy)\n    \n    print(f'Validation Loss: {np.mean(val_loss)}, Validation Accuracy: {np.mean(val_acc)}')\n\n# You can now save the model if needed\nbert_model.save_pretrained('./my_bert_model')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:01:50.919086Z","iopub.execute_input":"2024-09-15T04:01:50.919501Z","iopub.status.idle":"2024-09-15T04:29:25.960480Z","shell.execute_reply.started":"2024-09-15T04:01:50.919460Z","shell.execute_reply":"2024-09-15T04:29:25.959311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertForSequenceClassification\nimport numpy as np\n\n# Load the saved model\nloaded_model = TFBertForSequenceClassification.from_pretrained('./my_bert_model')\n\n# Prepare validation dataset\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': X_val_input_ids, 'attention_mask': X_val_attention_mask},\n    y_val\n)).batch(16)\n\n# Evaluate on validation set\nval_loss = []\nval_acc = []\nfor inputs, labels in val_dataset:\n    val_logits = loaded_model(inputs, training=False).logits\n    val_loss.append(loss_fn(labels, val_logits).numpy())\n    \n    # Calculate accuracy\n    predictions = np.argmax(val_logits, axis=-1)\n    accuracy = np.mean(predictions == labels.numpy())\n    val_acc.append(accuracy)\n\n# Print the final validation loss and accuracy\nprint(f'Validation Loss: {np.mean(val_loss)}, Validation Accuracy: {np.mean(val_acc)}')\n\n# Optionally, you can plot the metrics like before if you have metrics for all epochs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:38:36.385044Z","iopub.execute_input":"2024-09-15T04:38:36.385990Z","iopub.status.idle":"2024-09-15T04:38:57.427317Z","shell.execute_reply.started":"2024-09-15T04:38:36.385928Z","shell.execute_reply":"2024-09-15T04:38:57.426353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extracted data from your logs\nval_loss_values = [0.3915, 0.4275, 0.4970]\nval_acc_values = [0.8340, 0.8385, 0.8314]\n\n# Plot Validation Loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 4), val_loss_values, label='Validation Loss', marker='o')\nplt.title('Validation Loss Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xticks(range(1, 4))\nplt.legend()\nplt.show()\n\n# Plot Validation Accuracy\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 4), val_acc_values, label='Validation Accuracy', marker='o')\nplt.title('Validation Accuracy Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.xticks(range(1, 4))\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:39:55.421183Z","iopub.execute_input":"2024-09-15T04:39:55.421585Z","iopub.status.idle":"2024-09-15T04:39:55.951574Z","shell.execute_reply.started":"2024-09-15T04:39:55.421547Z","shell.execute_reply":"2024-09-15T04:39:55.950637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices({\n    'input_ids': X_test_input_ids,\n    'attention_mask': X_test_attention_mask\n}).batch(16)\n\n# Get predicted logits from the trained model\ntest_logits = bert_model.predict(test_dataset).logits\n\n# Convert logits to class labels (0 or 1 for binary classification)\ntest_predictions = tf.argmax(test_logits, axis=-1).numpy()\n\n# Prepare the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_df_filtered['id'],\n    'target': test_predictions\n})\n\n# Save the submission DataFrame as a CSV file\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:35:40.663474Z","iopub.execute_input":"2024-09-15T04:35:40.664481Z","iopub.status.idle":"2024-09-15T04:36:29.936936Z","shell.execute_reply.started":"2024-09-15T04:35:40.664435Z","shell.execute_reply":"2024-09-15T04:36:29.936084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nThe pretrained Bert model performed the best and also provided the highest score for the submission. Bidirectional nature of BERT can help achieve better results compared to the sequential approach of RNNS. That said, the RNN model also performed quite well, and could have been improved further. \n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}